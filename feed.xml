<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://mingyangzhang1995.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://mingyangzhang1995.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-06-29T22:46:51+00:00</updated><id>https://mingyangzhang1995.github.io/feed.xml</id><title type="html">Mingyang Zhang</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Continuous-time Finance Tools</title><link href="https://mingyangzhang1995.github.io/blog/continuous-time/" rel="alternate" type="text/html" title="Continuous-time Finance Tools"/><published>2024-06-16T00:00:00+00:00</published><updated>2024-06-16T00:00:00+00:00</updated><id>https://mingyangzhang1995.github.io/blog/continuous-time</id><content type="html" xml:base="https://mingyangzhang1995.github.io/blog/continuous-time/"><![CDATA[<h1 id="optimal-control">Optimal Control</h1> <p>Consider the below optimal control problem</p> \[\begin{aligned} \max_{u\in U} &amp; \int_{t_0}^{t_f} f(t,x_t, u_t)dt + h(x_{t_f})\\ s.t. \ &amp; dx_t = b(t,x_t,u_t)dt \end{aligned}\] <p>where</p> <p>\(x\in \mathbb{R}^n\), \(u \in U\)</p> <p>\(f:[t_0,t_f] \times \mathbb{R}^n \times U \rightarrow \mathbb{R}\),</p> <p>\(h:\mathbb{R}^n \rightarrow \mathbb{R}\),</p> <p>\(b:[t_0,t_f] \times \mathbb{R}^n \times U \rightarrow \mathbb{R}^n\).</p> <h2 id="maximum-principle">Maximum Principle</h2> <p>Let \(u^*:[t_0,t_f] \rightarrow U\) be an optimal control and \(x^*:[t_0,t_f]\rightarrow \mathbb{R}^n\) be the corresponding optimal state trajectory. Then there exist a function (called <em>costate</em>) \(p:[t_0,t_f]\rightarrow \mathbb{R}^n\) satisfying</p> \[H(t,x_t, u_t, p_t) = f(t,x_t, u_t) + \langle p_t, b(t,x_t, u_t)\rangle\] \[dp_t = -H_x(t,x^*_t, u^*_t, p_t)dt \text{ and } p_{t_f} = -K_x(x^*_{t_f})\] \[H(t,x^*_t, u^*_t,p_t) = \max_{u\in U} H(t,x^*_t, u_t,p_t)\] <h2 id="example-ramsey-model-1">Example: Ramsey model (1)</h2> \[\begin{aligned} \max_{u\in U} &amp; \int_{0}^{T} e^{-\rho t} u(c_t)dt\\ s.t. \ &amp; dW_t = -c_tdt + r^f_tW_tdt \end{aligned}\] <ul> <li> <p><strong>Step 1</strong>: write down the Hamiltonian:</p> \[H = e^{-\rho t} u(c_t) + p_t(-c_t + r^f_tW_t)\] </li> <li> <p><strong>Step 2</strong>: Take the first-order condition wrt the control \(c_t\)</p> \[H_c|_* = e^{-\rho t} u'(c^*_t) - p_t = 0 \Rightarrow p_t = e^{-\rho t} u'(c^*_t)\] </li> <li> <p><strong>Step 3</strong>: Take the derivative wrt the state variable \(W_t\)</p> \[dp_t = -p_tr_t^fdt = 0\] </li> <li> <p><strong>Step 4</strong>: Combine the equations in step 2 \&amp; 3, we get the Euler equation, where the pricing kernel is \(p_t\)</p> \[\frac{dp_t}{p_t} = -r_t^fdt \text{, } p_t = e^{-\rho t} u'(c^*_t)\] \[\rho -\frac{u''(c^*_t)}{u'(c^*_t)}c^{*'}_t = r_t^f\] </li> </ul> <p>For CRRA utility, the optimal trajectory \(W^*_t\) is described by (suppose \(r_t^f = r^f\))</p> \[\begin{cases} \gamma c_t' = (r^f-\rho)c_t \\ W_t' = -c_t + r^fW_t \end{cases}\] <h2 id="differential-equation">Differential Equation</h2> <p>We summarize the methods to solve the differential equations: analytical solution and numerical solution. We use the above differential equation in the Ramsey model as the example.</p> <h3 id="analytical-solution">Analytical Solution</h3> <p>Firstly, we rewrite the equation system in the matrix form \(X_t' = AX_t\). The general solution is \(X_t = e^{At}X_0\), where the exponential of a matrix is defined by the Taylor expansion (see definition in the <a href="https://en.wikipedia.org/wiki/Matrix_exponential">Wikipedia</a>).</p> \[\left[\begin{matrix} c\\ W \end{matrix}\right]' = \left[\begin{matrix} (\frac{r^f -\rho}{\gamma}) &amp; 0\\ -1 &amp; r^f \end{matrix}\right] \left[\begin{matrix} c\\ W \end{matrix}\right]\] <p>In order to compute the analytical solution of \(e^{At}\), we diagonalize the matrix \(A\) using the eigenvalue decomposition. Then, the exponential of \(A\) is \(e^{A} = Pe^{\Lambda}P^{-1}\). In our case, the eigenvalues are \((\frac{r^f -\rho}{\gamma})\) and \(r^f\), since the eigenvalues of a (lower/upper) diagonal matrix are the diagonal elements. Therefore, we have</p> \[\begin{aligned} e^{At} &amp;= \left[\begin{matrix} r^f - \frac{r^f -\rho}{\gamma} &amp; 0\\ 1 &amp; 1 \end{matrix}\right] \left[\begin{matrix} e^{(\frac{r^f -\rho}{\gamma})t} &amp; 0\\ 0 &amp; e^{r^ft} \end{matrix}\right] \left[\begin{matrix} r^f - \frac{r^f -\rho}{\gamma} &amp; 0\\ 1 &amp; 1 \end{matrix}\right]^{-1}\\ &amp;= \frac{1}{r^f - \frac{r^f -\rho}{\gamma}} \left[\begin{matrix} e^{(\frac{r^f -\rho}{\gamma})t}\left[r^f - \frac{r^f -\rho}{\gamma}\right] &amp; 0\\ e^{(\frac{r^f -\rho}{\gamma})t}-e^{r^ft} &amp; e^{r^ft}\left[r^f - \frac{r^f -\rho}{\gamma}\right] \end{matrix}\right] \end{aligned}\] <p>By the first row, we have the initial value equation of \(c(t)\)</p> <p>\(c(t) = e^{(\frac{r^f -\rho}{\gamma})t}c_0\).</p> <p>By the second row, we have the initial value equation of \(W(t)\)</p> <p>\((r^f - \frac{r^f -\rho}{\gamma})W(t) = \left(e^{(\frac{r^f -\rho}{\gamma})t}-e^{r^ft}\right)c_0 + e^{r^ft}\left[r^f - \frac{r^f -\rho}{\gamma}\right]W_0\).</p> <p>For general initial value differential equation, we are done. However, in our case, we do not have \(c_0\).</p> <p>For the finite-horizon Ramsey problem, we plug in \(W(T) = 0\) to the second equation to calculate \(c_0\), and then calculate the trajectory of \(c(t)\) and \(W(t)\).</p> <p>For the infinite-horizon Ramsey problem, we take $W(T) = 0$ and \(T \rightarrow \infty\).</p> \[\begin{aligned} \lim_{T \rightarrow \infty} \left(e^{(\frac{r^f -\rho}{\gamma})T}-e^{r^fT}\right)c_0 + e^{r^fT}\left[r^f - \frac{r^f -\rho}{\gamma}\right]W_0 &amp; = 0\\ \lim_{T \rightarrow \infty} \left(e^{(\frac{r^f -\rho}{\gamma}-r^f)T}-1\right)c_0 + \left[r^f - \frac{r^f -\rho}{\gamma}\right]W_0 &amp; = 0 \end{aligned}\] <p>\(r^f-\rho&lt;r^f\) and \(\gamma&gt;1\) \(\Rightarrow\) \(\frac{r^f -\rho}{\gamma}-r^f&lt;0\)</p> <p>\(\Rightarrow c_0 =\left(r^f - \frac{r^f -\rho}{\gamma}\right)W_0\).</p> <p>Plug in the initial value of \(c_0\) back to the dynamic of \(W(t)\), we have a simpler form. We summarize the solution in the below equation system</p> \[\begin{cases} W(t) = e^{(\frac{r^f -\rho}{\gamma})t}W_0\\ c(t) = e^{(\frac{r^f -\rho}{\gamma})t}c_0\\ c_0 =\left(r^f - \frac{r^f -\rho}{\gamma}\right)W_0\\ \end{cases}\] <p>It is not hard to see the below proposition.</p> <p><strong>Proposition</strong>: <em>under CRRA utility, the consumption \(c(t)\) is always proportional to the wealth \(W(t)\)</em></p> \[\frac{c_t}{W_t} = \frac{c_0}{W_0} = r^f - \frac{r^f -\rho}{\gamma}\] <p><em>Especially, for log utility, we have \(\frac{c_t}{W_t} = \frac{c_0}{W_0} = \rho\)</em></p> <h3 id="numerical-solution">Numerical solution</h3> <p>We summarize the very basic finite difference method. The function is \(W(t)\) (\(t \in (0,T)\)) is approximated by a vector \(W\) with length \(n\), with each element \(W_i\) in the vector corresponding to the approximation of \(W(t_i)\), where \(t_i\) are equally spaced between \(0\) and \(T\) (denoted as \(\Delta t\)). The first order derivative of \(W(t)\) is approximated by</p> \[W'(t) = \frac{W_{i+1} - W_{i-1}}{2\Delta t}\] <p>The second order derivative of \(W(t)\) is approximated by</p> \[W''(t) = \frac{W_{i+1} - 2W_{i} + W_{i-1}}{\Delta t^2}\] <p>For the finite-horizon Ramsey model, after rearranging the differential equation system of \(W(t)\) and \(c(t)\), we have the below differential equation of \(W(t)\)</p> \[(\gamma r^f+r^f-\rho)W_t^{*'}-\gamma W_t^{*''}-(r^f-\rho)r^fW_t^*=0\] <p>The above differential equation can be rewrite as a equation of \(W\), with \(W' = D_1 W\) and \(W'' = D_2 W\).</p> \[(\gamma r^f+r^f-\rho) D_1 W_t^{*}-\gamma D_2 W_t^{*}-(r^f-\rho)r^fW_t^*=b\] \[D_1 = \frac{1}{2\Delta t} \begin{bmatrix} 0 &amp; 1 &amp; &amp; &amp; &amp;\\ -1&amp; 0 &amp; 1 &amp; &amp; &amp;\\ &amp; -1&amp; 0 &amp; 1 &amp; &amp;\\ &amp; &amp; &amp;\ddots&amp; &amp;\\ &amp; &amp; &amp; -1 &amp; 0 &amp; 1 &amp;\\ &amp; &amp; &amp; &amp; -1 &amp; 0 &amp; \end{bmatrix}, D_2 = \frac{1}{\Delta t^2} \begin{bmatrix} -2 &amp; 1 &amp; &amp; &amp; &amp;\\ 1&amp; -2 &amp; 1 &amp; &amp; &amp;\\ &amp; 1&amp; -2 &amp; 1 &amp; &amp;\\ &amp; &amp; &amp;\ddots&amp; &amp;\\ &amp; &amp; &amp; 1 &amp; -2 &amp; 1 &amp;\\ &amp; &amp; &amp; &amp; 1 &amp; 2 &amp; \end{bmatrix}\] <p>Note: the right hand side of the equation is not \(0\), since the initial value and the terminal value of \(W'\) and \(W''\) is not correctly approximated by \(D_1 W\) and \(D_2 W\). Therefore, we have the first and the last element of \(b\) to be</p> <p>\(b_0 = -(-(\gamma r^f + r^f - \rho)/(2\Delta t) - \gamma/(\Delta t^2))W_0\) \(b_T = -(-(\gamma r^f + r^f - \rho)/(2\Delta t) - \gamma/(\Delta t^2))W_T\)</p> <p>After we have the trajectory \(W(t)\), we can calculate \(c_0\) and then the optimal control \(c(t)\)</p> <h3 id="code">Code</h3> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/continuous_time.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <h2 id="hjb-equation">HJB Equation</h2> <p><strong>Bellman’s Principle of optimality:</strong></p> \[V(s, x(s))=\sup _{u(\cdot) \in \mathcal{V}(s, T]}\{ \int_s^{\hat{s}} f(t, x(t), u(t)) d t +V(\hat{s}, x(\hat{s}))\}, \quad \forall 0 \leq s \leq \hat{s} \leq T\] <p>We derive the HJB equation from the Bellman’s principle of optimality.</p> \[\left\{\begin{array}{l} x({\hat{s}}) = x(s) + b(s,x(s),u(s))(\hat{s}-s) + o(\hat{s}-s) \\ V(\hat{s},x(\hat{s})) = V(s,x(s)) + V_t(s,x(s))(\hat{s}-s) + \langle V_x(s,x(s)), b(s,x(s),u(s))(\hat{s}-s)\rangle + o(\hat{s}-s) \\ \int_s^{\hat{s}} f(t, x(t), u(t)) d t = f(t, x(s), u(s)) (\hat{s}-s)+ o(\hat{s}-s) \end{array}\right.\] <p>Replace the corresponding parts in the Bellman’s Principle of optimal by the above three equations. \(\begin{aligned} &amp;0=\sup _{u(\cdot) \in \mathcal{V}(s, T]} \{ f(t, x(s), u(s)) (\hat{s}-s) + V_t(s,x(s))(\hat{s}-s) + \langle V_x(s,x(s)), b(t,x(s),u(s))(\hat{s}-s)\rangle + o(\hat{s}-s)\} \\ &amp;0=\sup _{u(\cdot) \in \mathcal{V}(s, T]} \{ f(t, x(s), u(s)) + V_t(s,x(s)) + \langle V_x(s,x(s)), b(t,x(s),u(s))\rangle + \frac{o(\hat{s}-s)}{(\hat{s}-s)}\} \end{aligned}\)</p> <p>Take \(\hat{s}-s \rightarrow 0\), we have the HJB equation</p> \[0=\sup _{u(\cdot) \in \mathcal{V}(s, T]} \{ f(t, x(s), u(s)) + V_t(s,x(s)) + \langle V_x(s,x(s)), b(t,x(s),u(s))\rangle\}\] <p><strong>Hamilton-Jacobi-Bellman equation:</strong> Suppose \(V \in C^1\left([0, T] \times \mathbb{R}^n\right)\). Then \(V\) is a solution to the following terminal value problem of a first-order partial differential equation:</p> \[\left\{\begin{array}{l} v_t + \sup _{u \in U} H\left(t, x, u,v_x\right) = 0, \quad(t, x) \in[0, T) \times \mathbb{R}^n \\ \left.v\right|_{t=T}=h(x_T), \quad x \in \mathbb{R}^n \end{array}\right.\] <p>where</p> \[\begin{aligned} H(t, x, u, p) \triangleq &amp; f(t, x, u) + \langle p, b(t, x, u)\rangle \\ &amp; \forall(t, x, u, p) \in[0, T] \times \mathbb{R}^n \times U \times \mathbb{R}^n \end{aligned}\] <p>The <strong>verification technique</strong> in principle involves the following steps:</p> <ul> <li>Step 1. Solve the HJB equation to find the value function \(V(t, x)\).</li> <li> <p>Step 2. Find \(u^*(t, x)\) through</p> \[H\left(t, x, u^*,v_x\right) = \sup _{u \in U} H\left(t, x, u,v_x\right)\] </li> <li>Step 3. Solve \(\displaystyle \begin{cases} dx_t = b(t,x_t,u_t)dt \\ x(0) = x_0\end{cases}\) to get the optimal pair \((x^*,u^*)\)</li> </ul> <h2 id="example-ramsey-model-2">Example: Ramsey model (2)</h2> <p>Solve the HJB equation.</p> <p>\(H = e^{-\rho t} u(c_t) + p_t(-c_t + r^f_tW_t)\) \(\begin{cases} v_t(t,W_t) + \max_{c_t} \{ e^{-\rho t} u(c_t) + v_W(t,W_t)(-c_t + r^f_tW_t)\}=0\\ v(T, W_T) = 0 \end{cases}\)</p> <p>FOC (CRRA):</p> <p>\(e^{-\rho t} u'(c_t)= v_W(t,W_t)\) \(c_t = \left[e^{\rho t}v_W(t,W_t)\right]^{-\frac{1}{\gamma}}\)</p> <p>the PDE becomes</p> \[\begin{cases} v_t(t,W_t) + e^{-\rho t} u(\left[e^{\rho t}v_W(t,W_t)\right]^{-\frac{1}{\gamma}}) + v_W(t,W_t)(-\left[e^{\rho t}v_W(t,W_t)\right]^{-\frac{1}{\gamma}} + r^f_tW_t)\}=0\\ v(T, W_T) = 0 \end{cases}\] <h1 id="stochastic-optimal-control">Stochastic Optimal Control</h1> <p>Consider the below stochastic optimal control problem</p> \[\begin{aligned} \max_{u\in U} \ &amp; E_t\int_{t_0}^{t_f} f(t,x_t, u_t)dt + h(x_{t_f})\\ s.t. \ &amp; dx = b(t,x_t,u_t)dt + \sigma(t,x_t,u_t) dB_t \end{aligned}\] <p>where</p> <p>\(x\in \mathbb{R}^n\), \(u \in U\), \(B\) is a \(m\)-dimension brownian motion.</p> <p>\(f:[t_0,t_f] \times \mathbb{R}^n \times U \rightarrow \mathbb{R}\).</p> <p>\(h:\mathbb{R}^n \rightarrow \mathbb{R}\).</p> <p>\(b:[t_0,t_f] \times \mathbb{R}^n \times U \rightarrow \mathbb{R}^n\).</p> <p>\(\sigma: [t_0,t_f] \times \mathbb{R}^n \times U \rightarrow \mathbb{R}^{n\times m}\).</p> <h2 id="stochastic-maximum-principle">Stochastic Maximum Principle</h2> <p>In stochastic optimal control, we have four costates \((p_t, q_t)\) and \((P_t, Q_t)\), where \((P_t, Q_t)\) can be considered as the second order costates. Meanwhile, we do not take the maximization of the Hamiltonian. Instead, we take the maximization of the $\mathcal{H}$-function. Besides, we introduce the <em>generalized Hamiltonian</em> \(G\) for simplicity.</p> \[\displaystyle \begin{aligned} G(t,x_t,u_t,p_t,P_t) &amp;= H(t,x_t,u_t,p_t) + \frac{1}{2}tr[\sigma(t,x_t,u_t)'P_t\sigma(t,x_t,u_t)]\\ H(t,x_t,u_t,p_t,q_t) &amp;= H(t,x_t,u_t,p_t) + tr[q_t'\sigma(t,x_t,u_t)]\\ \mathcal{H}(t,x_t,u_t) &amp;= G(t,x_t,u_t,p_t,P_t) + tr\{\sigma(t,x_t,u_t)'[q_t -P_t\sigma(t,x^*_t,u^*_t)]\}\\ &amp;(t,x,u,p,q,P) \in [t_0,t_f] \times \mathbb{R}^n \times U \times \mathbb{R}^n \times \mathbb{R}^{n \times m} \times \mathcal{S}^n \end{aligned}\] <p><strong>Stochastic Maximum Principle:</strong> There exist pairs of process \((p_t, q_t)\) and \((P_t, Q_t)\) satisfying</p> \[\begin{cases} dp_t = -H_x(t,x_t,u_t,p_t,q_t)dt + q_t dB_t\\ p_{t_f} = -h_x(x^*_{t_f})\\ \end{cases}\] \[\begin{cases} dP_t =&amp; -\{b_x(t,x^*_t,u^*_t)P_t + P_tb_x(t,x^*_t,u^*_t)\\ &amp;+\sum_{j=1}^m\sigma_x^j(t,x^*_t,u^*_t)'P_t\sigma_x^j(t,x^*_t,u^*_t) \\ &amp;+ \sum_{j=1}^m[\sigma_x^j(t,x^*_t,u^*_t)'Q^j_t + Q^j_t\sigma_x^j(t,x^*_t,u^*_t)]\\ &amp;+ H_{xx}(t,x^*_t,u^*_t,p_t, q_t)\}dt + \sum_{j=1}^mQ^j_tdB^j_t\\ P_{t_f} =&amp; -h_{xx}(x^*_{t_f})\\ \end{cases}\] <p>such that</p> \[\mathcal{H}(t,x^*_t,u^*_t) = \max_{u\in U}\mathcal{H}(t,x^*_t,u_t)\] <p><strong>Special Case:</strong> when the diffusion term does not contain the control variable, i.e. \(\sigma(t,x_t,u_t) = \sigma(t,x_t)\), the costates \((P_t, Q_t)\) drops. The maximum condition of \(\mathcal{H}\) reduce to</p> \[H(t,x^*_t, u^*_t,p_t,q_t) = \max_{u\in U} H(t,x^*_t, u_t,p_t,q_t)\] <h2 id="example-lucas-tree">Example: Lucas Tree</h2> \[\begin{aligned} \max_{c,\theta}\ &amp; E_t\int_{0}^{\infty} e^{-\rho t} u(c_t)dt\\ s.t. \ &amp; dW_t = -c_tdt + r^f_tW_tdt + W_t\theta_t'(dr_t-r^f_tdt)\\ &amp; dr_t = \mu_t dt + \sigma_t dB_t \\ &amp; (\sigma_t\in \mathbb{R}^{s\times m}, B_t\in \mathbb{R}^{m},\mu_t \in \mathbb{R}^s, \theta_t \in \mathbb{R}^s)\\ &amp; \Rightarrow dW_t = \left[-c_t + r^f_tW_t + W_t\theta_t'(\mu_t-r^f_t)\right]dt + W_t\theta_t'\sigma_tdB_t \end{aligned}\] <ul> <li> <p><strong>Step 1</strong>: write down the Hamiltonian, the generalized Hamiltonian and the \(\mathcal{H}\)-function \(\displaystyle \begin{aligned} G &amp;= e^{-\rho t} u(c_t) + p_t\left[-c_t + r^f_tW_t + W_t\theta_t'(\mu_t-r^f_t)\right] + \frac{1}{2}tr\left[(W_t\theta_t'\sigma_t)'P_t(W_t\theta_t'\sigma_t)\right]\\ H &amp;= e^{-\rho t} u(c_t) + p_t\left[-c_t + r^f_tW_t + W_t\theta_t'(\mu_t-r^f_t)\right] + tr[q_t'(W_t\theta_t'\sigma_t)]\\ \mathcal{H} &amp;= G + tr\{(W_t\theta_t'\sigma_t)'[q_t -P_t(W^*_t\theta_{t}^{*'}\sigma_t^*)]\} \end{aligned}\)</p> </li> <li> <p><strong>Step 2</strong>: Take the first-order condition of the \(\mathcal{H}\)-function wrt \(c\) and \(\theta\)</p> </li> </ul> <p>Note the derivative of the trace is as follow</p> \[\begin{aligned} d \{tr\left[(W_t\theta_t'\sigma_t)'P_t(W_t\theta_t'\sigma_t)\right] \}&amp;= W_t^2P_t\{d\left[tr(\sigma_t'\theta_t\theta_t\sigma_t)\right]\}\\ &amp;= W_t^2P_ttr[d(\sigma_t'\theta_t\theta_t\sigma_t)]\\ &amp;= 2W_t^2P_ttr[\theta_t'\sigma_t\sigma_t'd\theta_t]\\ &amp;= 2W_t^2P_t\theta_t'\sigma_t\sigma_t'd\theta_t \ \ (\text{since }\theta_t'\sigma_t\sigma_t'd\theta_t\text{ is a scalar})\\ d \{tr\left[(W_t\theta_t'\sigma_t)'P_t(W^*_t\theta_t^{*'}\sigma_t)\right] \}&amp;= W_tW^*_tP_t\theta_t^{*'}\sigma_t\sigma_t'd\theta_t\\ d\{tr[(W_t\theta_t\sigma_t)'q_t]\} &amp;=W_ttr[q_t\sigma_t'd\theta_t]\ \ (q\in \mathbb{R}^{1\times m})\\ &amp;=W_tq_t\sigma_t'd\theta_t \end{aligned}\] <p>Therefore, the FOC’s are</p> \[\begin{aligned} \mathcal{H}_c|_* &amp;= e^{-\rho t} u'(c^*_t) - p_t = 0\\ \mathcal{H}_{\theta}|_* &amp;= p_tW^*_t(\mu_t -r^f_t) +W_t^2P_t\sigma_t\sigma_t'\theta^*_t + W^*_t\sigma_tq_t' - W^{*2}_tP_t\sigma_t\sigma_t'\theta_t^*\\ &amp; = p_tW^*_t(\mu_t -r^f_t) + W^*_t\sigma_tq_t' =0 \\ (\mathcal{H}_{\theta} &amp;= p_tW_t(\mu_t -r^f_t) +W_t^2P_t\sigma_t\sigma_t'\theta_t + W_t\sigma_tq_t' - W_tW^*_tP_t\sigma_t\sigma_t'\theta_t^*) \end{aligned}\] <p>Here, \((P_t,Q_t)\) are cancelled out in the FOC’s.</p> <p>Suppose \(s = m\), i.e., the market is complete. We have</p> \[\begin{aligned} p_t &amp;= e^{-\rho t} u'(c^*_t)\\ q_t' &amp;=-p_t \sigma_t^{-1}(\mu_t - r_t^f) \end{aligned}\] <ul> <li><strong>Step 3</strong>: Take the derivative of the Hamiltonian \(H\) wrt the state variable \(W_t\) and then write down the adjoint equation for \(p_t\)</li> </ul> \[\begin{aligned} dp_t &amp;= -H_xdt + q_tdB_t\\ &amp;= -\left\{p_t\left[r^f_t + \theta_t'(\mu_t-r^f_t)\right] + tr\left[q_t'\theta_t'\sigma_t\right]\right\}dt + q_tdB_t \end{aligned}\] <ul> <li><strong>Step 4</strong>: Combine the equations in step 2 &amp; 3, we get the Euler equation, where the pricing kernel is \(p_t\)</li> </ul> \[\begin{aligned} dp_t&amp;= -p_t\left\{\left[r^f_t + \theta_t'(\mu_t-r^f_t)\right] + tr\left[\sigma_t^{-1}(\mu_t - r_t^f) \theta_t'\sigma_t\right]\right\}dt - p_t (\sigma_t^{-1}(\mu_t - r_t^f))' dB_t\\ &amp; = -p_tr^f_tdt- p_t(\sigma_t^{-1}(\mu_t - r_t^f))'dB_t\\ \end{aligned}\] \[\frac{dp_t}{p_t} = -r^f_tdt - (\sigma_t^{-1}(\mu_t - r_t^f))' dB_t\] <h2 id="pricing-kernel">Pricing Kernel</h2> <p><strong>Definition</strong>: <em>In continuous time, a pricing kernel w.r.t. probability space \((\Omega,\mathcal{F},\mathbb{P})\) is defined as an adapted process \(\Lambda_t\), such that \(\Lambda_t P_t\) is a martingale for any asset with price \(P_t\)</em>.</p> <p>The below expression are equivalent to each other (\(s&gt;t\)):</p> <ol> <li> \[\displaystyle E(\Lambda_s P_s | \mathcal{F}_t) = \Lambda_t P_t\] </li> <li> \[\displaystyle E(d(\Lambda_t P_t) | \mathcal{F}_t) = 0\] </li> <li> \[\displaystyle E(\frac{d(\Lambda_t P_t)}{\Lambda_t P_t} | \mathcal{F}_t) = 0\] </li> </ol> <p>We check the Euler equation \(\displaystyle E_t\left(\frac{d(p_tr_t)} {p_t} \right) = 0\).</p> \[\begin{aligned} E_t\left(\frac{d(p_tr_t)} {p_t} \right) &amp;= E_t\left(\frac{dp_t} {p_t} + dr_t + \frac{dp_t} {p_t}dr_t \right)\\ &amp;= -r^f_tdt + \mu_tdt +E_t\left(\left[- (\sigma_t^{-1}(\mu_t - r_t^f))'dB_t\right]\sigma_tdB_t\right)\\ &amp;= -r^f_tdt + \mu_tdt +E_t\left(\left[- dB_t'\sigma_t^{-1}(\mu_t - r_t^f)\right]\sigma_tdB_t\right)\\ &amp;= -r^f_tdt + \mu_tdt +E_t\left(- \sigma_tdB_tdB_t'\sigma_t^{-1}(\mu_t - r_t^f)\right)\\ &amp;= -r^f_tdt + \mu_tdt - \sigma_t\sigma_t^{-1}(\mu_t - r_t^f)dt\\ &amp;= 0 \end{aligned}\] <h2 id="stochastic-hjb-equation">Stochastic HJB Equation</h2> <p><strong>Bellman’s Principle of optimality (stochastic version):</strong></p> \[V(s, x(s))=\sup _{u(\cdot) \in \mathcal{V}(s, T]} E \{ \int_s^{\hat{s}} f(t, x(t), u(t)) d t +V(\hat{s}, x(\hat{s}))\}, \quad \forall 0 \leq s \leq \hat{s} \leq T\] <p>Similarly, we derive the stochastic HJB from the Bellman’s Principle of optimality. For simplicity, we write \(\hat{s} = s +dt\)</p> \[\begin{aligned} V(\hat{s},x(\hat{s})) &amp; = V(s,x(s)) + V_t(s,x(s))dt + \langle V_x(s,x(s)),dx(s)\rangle + \frac{1}{2}dx(s)'V_{xx}(s,x(s))dx(x)\\ V(\hat{s},x(\hat{s})) &amp; = V(s,x(s)) + V_t(s,x(s))dt + \langle V_x(s,x(s)),dx(s)\rangle + \frac{1}{2}tr\left[\sigma(s,x_s,u_s)'V_{xx}(s,x(s))\sigma(s,x_s,u_s)\right]dt \end{aligned}\] \[\int_s^{\hat{s}} f(t, x(t), u(t)) d t = f(t, x(s), u(s)) dt\] <p>We have the HJB equation</p> \[\begin{aligned} 0 &amp;= \sup _{u(\cdot) \in \mathcal{V}(s, T]} E \{ f(s, x(s), u(s)) dt +V_t(s,x(s))dt + \langle V_x(s,x(s)),dx(s)\rangle + \frac{1}{2}tr\left[\sigma(s,x_s,u_s)'V_{xx}(s,x(s))\sigma(s,x_s,u_s)\right]dt\}\\ 0 &amp;= \sup _{u(\cdot) \in \mathcal{V}(s, T]} \{ f(s, x(s), u(s)) dt +V_t(s,x(s))dt + \langle V_x(s,x(s)),b(t,x(s),u(s))\rangle dt + \frac{1}{2}tr\left[\sigma(s,x_s,u_s)'V_{xx}(s,x(s))\sigma(s,x_s,u_s)\right]dt\}\\ 0 &amp;= \sup _{u(\cdot) \in \mathcal{V}(s, T]} \{ f(s, x(s), u(s)) dt +V_t(s,x(s)) + \langle V_x(s,x(s)),b(t,x(s),u(s))\rangle + \frac{1}{2}tr\left[\sigma(s,x_s,u_s)'V_{xx}(s,x(s))\sigma(s,x_s,u_s)\right]\} \end{aligned}\] <p><strong>Hamilton-Jacobi-Bellman equation:</strong> Suppose \(V \in C^{1,2}\left([0, T] \times \mathbb{R}^n\right)\). Then \(V\) is a solution to the following terminal value problem of a (possibly degenerate) second-order partial differential equation:</p> \[\left\{\begin{array}{l} v_t + \sup _{u \in U} G\left(t, x, u,v_x,v_{xx}\right) = 0, \quad(t, x) \in[0, T) \times \mathbb{R}^n, \\ \left.v\right|_{t=T}=h(x_T), \quad x \in \mathbb{R}^n, \end{array}\right.\] <p>where</p> \[\begin{aligned} G(t, x, u, p, P) \triangleq &amp; H(t, x, u, p) +\frac{1}{2}tr[\sigma(t,x,u)'P_t\sigma(t,x,u)]\\ H(t, x, u, p) \triangleq &amp; f(t, x, u) + \langle p, b(t, x, u)\rangle \\ &amp; \forall(t, x, u, p) \in[0, T] \times \mathbb{R}^n \times U \times \mathbb{R}^n \end{aligned}\]]]></content><author><name></name></author><summary type="html"><![CDATA[This notes summarizes useful continuous finance tools and methods.]]></summary></entry><entry><title type="html">Replication: Long-run Risk Model</title><link href="https://mingyangzhang1995.github.io/blog/long-run-risk/" rel="alternate" type="text/html" title="Replication: Long-run Risk Model"/><published>2024-06-16T00:00:00+00:00</published><updated>2024-06-16T00:00:00+00:00</updated><id>https://mingyangzhang1995.github.io/blog/long-run-risk</id><content type="html" xml:base="https://mingyangzhang1995.github.io/blog/long-run-risk/"><![CDATA[<p><strong>I gratefully thank the help of my supervisor <a href="https://www.davidschreindorfer.com/">David Schreindorfer</a>. The Python code was built based on the replication code in <a href="https://www.journals.uchicago.edu/doi/10.1086/720396">Beason and Schreindorfer (2022)</a> and the Macro-finance lecture notes by David Schreindorfer at ASU. All faults are mine.</strong></p> <p>The below code uses PyTorch to numerically replicates the long-run risk model and related papers. With cuda, the parallel computation highly speed up the matrix calculation.</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/replication_long_run_risk_torch.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><summary type="html"><![CDATA[This notes replicates the long-run risk model numerically.]]></summary></entry></feed>